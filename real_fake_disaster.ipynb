{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real or Not? NLP with Disaster Tweets\n",
    "\n",
    "Predict which Tweets are about real disasters and which ones are not.\n",
    "> We are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./tweet/train.csv')\n",
    "test = pd.read_csv('./tweet/test.csv')\n",
    "submission = pd.read_csv('./tweet/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "target = train['target']\n",
    "sns.countplot(target)\n",
    "train.drop(['target'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated train and test set\n",
    "def concate_df(train, test):\n",
    "    return pd.concat([train,test], sort=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = concate_df(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 4)\n",
      "(3263, 4)\n",
      "(10876, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(df_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...\n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada\n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...\n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...\n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10876.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5437.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3139.775098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2718.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5437.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8156.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10875.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id\n",
       "count  10876.000000\n",
       "mean    5437.500000\n",
       "std     3139.775098\n",
       "min        0.000000\n",
       "25%     2718.750000\n",
       "50%     5437.500000\n",
       "75%     8156.250000\n",
       "max    10875.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10876 entries, 0 to 10875\n",
      "Data columns (total 4 columns):\n",
      "id          10876 non-null int64\n",
      "keyword     10789 non-null object\n",
      "location    7238 non-null object\n",
      "text        10876 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 340.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of missing values in keyword is 87 for the combined dataset\n",
      "The number of missing values in keyword is 61 for the train dataset\n",
      "The number of missing values in keyword is 26 for the test dataset\n",
      "The number of missing values in location is 3638 for the combined dataset\n",
      "The number of missing values in location is 2533 for the train dataset\n",
      "The number of missing values in location is 1105 for the test dataset\n"
     ]
    }
   ],
   "source": [
    "features = ['keyword','location']\n",
    "for feat in features : \n",
    "    print(\"The number of missing values in \"+ str(feat)+\" is \"+str(df_all[feat].isnull().sum())\n",
    "          + \" for the combined dataset\")\n",
    "    print(\"The number of missing values in \"+ str(feat)+\" is \"+str(train[feat].isnull().sum())\n",
    "          + \" for the train dataset\")\n",
    "    print(\"The number of missing values in \"+ str(feat)+\" is \"+str(test[feat].isnull().sum())\n",
    "          + \" for the test dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# check if any keyword are missing in train set but present in test set\n",
    "keyw_train = train['keyword'].unique()\n",
    "keyw_test = test['keyword'].unique()\n",
    "print(set(keyw_train)==set(keyw_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train['text']\n",
    "train_size = int(7613*0.8)\n",
    "train_sentences = sentences[:train_size]\n",
    "train_labels = target[:train_size]\n",
    "\n",
    "test_sentences = sentences[train_size:]\n",
    "test_labels = target[train_size:]\n",
    "\n",
    "# parameters of tokenizer\n",
    "embedding_dim = 16\n",
    "vocab_size = 10000\n",
    "max_lenght = 120\n",
    "trunc_type = 'post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "padded = pad_sequences(sequences, maxlen=max_lenght, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_lenght, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_lenght),\n",
    "    tf.keras.layers.GlobalAvgPool1D(),\n",
    "    tf.keras.layers.Dense(14, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 14)                238       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 15        \n",
      "=================================================================\n",
      "Total params: 160,253\n",
      "Trainable params: 160,253\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, <class 'pandas.core.series.Series'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8b2879a16d3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    495\u001b[0m                      'at same time.')\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m   \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m   \u001b[0;31m# Handle validation_split, we want to split the data and get the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 653\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    654\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, <class 'pandas.core.series.Series'>"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(padded, train_labels, epochs=num_epochs, validation_data=(testing_padded, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-6083f9d95718>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(history,string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()\n",
    "plot(history, \"accuracy\")\n",
    "plot(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = []\n",
    "for val in pred_plot.target:\n",
    "    if val > 0.5:\n",
    "        final_output.append(1)\n",
    "    else:\n",
    "        final_output.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = final_output\n",
    "# submission['id'] = test['id']\n",
    "submission.to_csv(\"final.csv\", index=False)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
