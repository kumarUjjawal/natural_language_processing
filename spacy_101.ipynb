{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy Usage: \n",
    "1. Information Extraction\n",
    "2. Natural Language Understanding\n",
    "3. Depp Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features:\n",
    "1. Tokenization\n",
    "2. Lemmatization\n",
    "3. Part-of-Speech Tagging\n",
    "4. Dependency Parsing\n",
    "5. Sentence Boundry Detection\n",
    "6. Named Entity Recognition\n",
    "7. Entity Linking\n",
    "8. Similarity\n",
    "9. Text Classification\n",
    "10. Rule-based Matching\n",
    "11. Training\n",
    "12. Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DET det\n",
      "cat NOUN nsubj\n",
      "sat VERB ROOT\n",
      "on ADP prep\n",
      "the DET det\n",
      "mat NOUN pobj\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"The cat sat on the mat.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You\n",
      "do\n",
      "n't\n",
      "seem\n",
      "to\n",
      "have\n",
      "the\n",
      "spaCy\n",
      "package\n",
      "itself\n",
      "installed\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"You don't seem to have the spaCy package itself installed.\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech Tagging and Dependecies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Text: The original word text.\n",
    "    Lemma: The base form of the word.\n",
    "    POS: The simple part-of-speech tag.\n",
    "    Tag: The detailed part-of-speech tag.\n",
    "    Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "    Shape: The word shape – capitalization, punctuation, digits.\n",
    "    is alpha: Is the token an alpha character?\n",
    "    is stop: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You PRON PRP nsubj Xxx True True\n",
      "do AUX VBP aux xx True True\n",
      "n't PART RB neg x'x True False\n",
      "seem VERB VB ROOT xxxx True True\n",
      "to PART TO aux xx True True\n",
      "have AUX VB xcomp xxxx True True\n",
      "the DET DT det xxx True True\n",
      "spaCy NOUN NN compound xxxXx False True\n",
      "package NOUN NN nsubj xxxx False True\n",
      "itself PRON PRP appos xxxx True True\n",
      "installed VERB VBD ccomp xxxx False True\n",
      ". PUNCT . punct . False False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_, token.dep_, token.shape_, token.is_stop, token.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Text: The original entity text.\n",
    "    Start: Index of start of entity in the Doc.\n",
    "    End: Index of end of entity in the Doc.\n",
    "    Label: Entity label, i.e. type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vector and Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. has vector: Does the token have a vector representation?\n",
    "2. Vector norm: The L2 norm of the token’s vector (the square root of the sum of the values     squared)\n",
    "3. OOV: Out-of-vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lion True 6.5120897 False\n",
      "Tiger True 6.518183 False\n",
      "Milk True 7.3250523 False\n",
      "Honey True 6.845015 False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(\"Lion Tiger Milk Honey\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lion Lion 1.0\n",
      "Lion Tiger 0.7359829\n",
      "Lion Milk 0.2319908\n",
      "Lion Honey 0.27584368\n",
      "Tiger Lion 0.7359829\n",
      "Tiger Tiger 1.0\n",
      "Tiger Milk 0.20491147\n",
      "Tiger Honey 0.308199\n",
      "Milk Lion 0.2319908\n",
      "Milk Tiger 0.20491147\n",
      "Milk Milk 1.0\n",
      "Milk Honey 0.64480776\n",
      "Honey Lion 0.27584368\n",
      "Honey Tiger 0.308199\n",
      "Honey Milk 0.64480776\n",
      "Honey Honey 1.0\n"
     ]
    }
   ],
   "source": [
    "# similarity\n",
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "data = nlp.to_bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
